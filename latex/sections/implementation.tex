\section{Implementation}
The code for this work can be accessed over this git hub link: \url{https://github.com/valentino-dev/Bachelorthesis/tree/main/src}.
\subsection{Matrix representation}
For the implementation we construct the lattice and place all operators on thier links. Then we calculate each entry while we can directly use the fact, that we are in the electric basis:
\begin{align*}
	\bra{i}\hat{H}\ket{j} = & \frac{g^2}{2}\sum_{\vec{r}}\left(e_{\vec{r}, x}^2+e_{\vec{r}, y}^2\right)\delta_{ij}           \\
	                        & -\frac{1}{2a^2g^2}\sum_{\vec{r}}\bra{i}\left(\hat{P}_{\vec{r}}+\hat{P}_{\vec{r}}\right)\ket{j}
\end{align*}
with $\ket{i}\in\cal{H}$. Let us calculate $\bra{i}\hat{P}_{\vec{r}}\ket{j}$:
\begin{align*}
	\bra{i}\hat{P}_{\vec{r}}\ket{j}= & \bra{i}\hat{U}_{\vec{r}, x}\hat{U}_{\vec{r}+x,y}\hat{U}^{\dag}_{\vec{r}+y,x}\hat{U}^{\dag}_{\vec{r},y}\ket{j} \\
	=                                & \bra{i}\ket{e^{(j)}_{\vec{r}, x}+1}\otimes\ket{e^{(j)}_{\vec{r}+x, y}+1}                                      \\
	                                 & \otimes\ket{e^{(j)}_{\vec{r}+y, x}-1}\otimes\ket{e^{(j)}_{\vec{r}, y}-1}                                      \\
	                                 & \bigotimes_\text{rest links}\ket{e^{(j)}_{\vec{r}', \mu'}}                                                    \\
	=                                & \Braket{i|k}                                                                                                  \\
	=                                & \delta_{ik}
\end{align*}
The formulation reads as following: State $\ket{j}$ will be transformed by the plaquette operator $\hat{P}_{\vec{r}}$ into some state $\ket{k}$. Thus $\hat{P}_{\vec{r}}$ gets an entry at row $\bra{i}$ and collumn $\ket{j}$ if, and only if, state $\ket{j}$ is transformed into state $\ket{i}$. Now knowing $\hat{P}_{\vec{r}}$ in matrix representation, trivialy yealds $\hat{P}_{\vec{r}}^{\dag}=\left(\hat{P}_{\vec{r}}^{*}\right)^{T}$ and with it the matrix representation of the magnetic hamiltonian.

On a side note, going through states $\ket{i}$ means, counting up in base $2l+1$ with the link states $\ket{e_{\vec{r}, \mu}}$ being the "digits". This is schematicly illustrated in \cref{tab:steidx}.

\begin{table}[h]
		\begin{tabular}{c|c}
			$i$      & $\ket{i}$               \\
			\hline
			$0$      & $(-l,-l,\dots, -l,-l)$  \\
			$1$      & $(-l,-l,\dots,-l,-l+1)$ \\
			$\vdots$ & $\vdots$                \\
			$2l+1$   & $(-l,-l,\dots,-l,l)$    \\
			$2l+2$   & $(-l,-l,\dots,-l+1,l)$
		\end{tabular}
		\caption{Scheme of state indexing.}\label{tab:steidx}
\end{table}

With this proccedure we would have to check every combination of states $\bra{i}$ and $\ket{j}$, which would be very costly. Instead we just calculate the transformation $\hat{P}_{\vec{r}}\ket{j}=\ket{k}$ for every state $\ket{j}$ and set $(\hat{P}_{\vec{r}})_{kj}=1$, i.e. having a contribution at row $\bra{k}$ and collumn $\ket{j}$.

Gauss's Law not only restricts the electric operators, but also the corresponding link operator on the same link. But here Gauss's Law does not impose a dependency, but rather the dynamical link operators automticly produce physical states, where as the fixed link operators do not act anymore on our physical space, which is why we can ignore the fixed ones. Thus with plaquette operators we would normaly have allways four link operators, we now get plaquette operators that are a product of any number of link operators ranging from 0 to 4. Which number it will be is then dependent on the position of the plaquette. When a plaquette does not go throught any dynamical link operators, it will never produce a physical state and thus can be entirely ignored.

\subsection{Exact diagonalisation}
Now having the total hamiltonian in matrix representation, we can use diagonalisation to compute the eigenvalues and eigenstates. For that the library \texttt{scipy}\cite{2020SciPy-NMeth} was used. It provides the method \texttt{scipy.sparse.linalg.eigsh}, which is an eigensolver, which can be used to calculate the $k$ smallest algebraic (SA) eigenvalues. It can use hermitian sparse row matrices which speed up the process drasticly in comperison to dense non hermitian matrices.

\subsection{Computational ressources}
The disadvantage of the hamiltonian formulation of the lattice gauge theory, is it need for computational ressources, since every link can be in $2l+1$ states and in two dimensions, we have two links for every site. Now depending if we have periodic boundary conditions (PBC) or not, we have the links that connect the sites of opposides sides or not.
The exact number of states for a square $n \cross n$ lattice with PBC would be $N=(2l+1)^{2n^2}$ and without PBC $N=(2l+1)^{2(n^2-n)}$. But since we are only interested in physical states, each site imposes a Gauss's Law and thus restricts the number of total states to only the physical states. So each site introduces a constraint
\begin{align}
	\sum_{\mu=x,y}\left(E_{\vec{r}, \mu} - E_{\vec{r}, -\mu}\right) = Q_{\vec{r}}.
\end{align}
To check if they are linearly independend we sum over all sites and get
\begin{align}
	\sum_{\vec{r}}\sum_{\mu=x,y}\left(E_{\vec{r}, \mu} - E_{\vec{r}, -\mu}\right) = \sum_{\vec{r}}Q_{\vec{r}}.
\end{align}
We see, that only if the total sum of charges is non zero, we have linear independency. But if the total sum is zero, we have a redundent constraint, which reduces the total number of constraints from $n^2$ to $n^2 -1$. From now on we assume that the total charge is allways zero, since we only work with no charges or with a pair of opposite charges.

This limits our total number of electric operators $N_{E}=2n^2$ to only a fraction that is dynamic: $N_{E,\text{dyn}} = n^2+1$. For a lattice without PBC we get $N_{E}=2(n^2-n)$ and $N_{E,\text{dyn}} = n^2-2n+1$.
The number of physical states for a lattic with PBC is thus
\begin{align}
	N_{\text{ph}}=(2l+1)^{n^2+1}.
\end{align}
And without PBC
\begin{align}
	N_{\text{ph}}=(2l+1)^{n^2-2n+1}.
\end{align}
Since from our calculations we will receive matrices with size $N_{\text{ph}} \cross N_{\text{ph}}$, the number of physical states will be a good measure for computation time.
To get an idea on some realistic lattices and thier number of physical states, see \cref{tab:num}.

\begin{table}[h]
	\begin{tabular}{c|c}
		lattice               & $N_{\text{ph}}$ \\
		\hline
		2x2, no PBC and $l=1$ & \num{3}         \\
		2x2, PBC and $l=1$    & \num{243}       \\
		2x2, PBC and $l=7$    & \num{759e3}     \\
		3x3, PBC and $l=1$    & \num{19.7e3}    \\
		3x3, PBC and $l=2$    & \num{1.95e6}    \\
		3x3, PBC and $l=4$    & \num{387e6}
	\end{tabular}
	\caption{Lattice sizes and thier number of physical states.}\label{tab:num}
\end{table}

We can use to our advantage, that most of the elements of the hamiltonian are zero and only a few are non-zero entries. Thus instead of storing all elements, even those that are zero, we only store the non-zero entries by storing its row and collumn position and the value. This is called a Compressed Sparse Row (CSR) matrix and will reduce the needed memory drasticly.\footnote{Nevertheless a Hamiltonian of a $3\cross3$ lattice with PBC and $l=3$ takes about \SI{150}{GB} to be stored.}

Now that we have a little intuition for the complexity, we continue with the actual computation times. We do the calculations on the supercomputer Marvin of the University of Bonn. Building the Hamiltonian for the $3\cross 3$ lattice with PBC and $l=1$ took \SI{1.5}{s} and diagonalizing it to $g=1$ took \SI{0.27}{s}.\footnote{This assessment was done by using one node with two CPUs of the type Intel Xeon 'Sapphire Rapids' 48-core/96-thread 2.10GHz.} B

\begin{table}[h]
	\begin{tabular}{c|c|c}
		truncation $l$ & building $\hat{H}$ & diagonalizing    \\
		\hline
		1              & \SI{1.5}{s}        & \SI{0.27}{s}     \\
		2              & \SI{110}{s}        & \SI{11}{minutes} \\
		3              & \SI{1}{h}          & \SI{5}{h}
	\end{tabular}
	\caption{Computation time for a $3\cross 3$ lattice with PBC.}\label{tab:times}
\end{table}

To utilize the HPC to capacity, multiprocessing was introduced. The code was rewriten, such that the calculation of the elements is spreaded onto the threads, without spreading to thinly, i.e. launching new threads takes more time then processing, or to dense, i.e. not all cores are utilized.
